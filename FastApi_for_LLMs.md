# Setting a fastAPI end point for LLMs

## Introduction

Hey everyone, welcome to this video where we're going to build an AI endpoint using FastAPI. If you've been playing around with LLMs like ChatGPT or Gemini, and you have an idea for an app, then the next step is often figuring out how you can make that accessible to other users. Building an API is probably the best way to do this. And here's a demo of the endpoint that we're going to build by the end of this video. This is just going to be a normal REST API endpoint that we are hosting locally on our machine. Users can chat with it, and it also rate-limits us to prevent users from overloading or abusing your service. This is going to be a great starting point for any custom AI application that you'd eventually want to build. To do all of that, we're going to start by setting up a basic FastAPI server in Python, and then connect it to an AI model like Gemini. Then we'll look at adding rate-limiting to protect your endpoint from overload or abuse. Finally, I'll give you a quick outline with simple examples for how you can implement user authentication using JWT tokens. And I think all of this is going to be the bare bones minimum setup that you need if you want to create an API endpoint that uses AI. All the code for this project is available at this GitHub link. The link is going to be in the video description. Alright, let's begin. Before we get into the code, let's quickly review the high-level architecture.

## Project Overview

Honestly, this project is going to be pretty simple. We're building a very basic API backend that exposes an endpoint to our users. The endpoint is just a generic chat endpoint, so the user can send whatever text they want, and the backend itself will use an AI model to answer. I've chosen Python and FastAPI as the framework here because it's simple and easy to start with. For the AI model, I've chosen Gemini because the latest Flash model is cheap and powerful, and we will need our own Gemini API key to use it, and we'll store that in the backend. And if you wanted to use a different AI platform with this tutorial, then no worries, all you have to do is change one file in this project. Finally, as part of the backend, we'll also implement a rate limiter to prevent users from sending too many requests. Because we are the ones paying for this API key, we'll want a lot of control over the amount of traffic we can let through, both globally and also per authenticated user. Okay, now let's move on to setting up the project.

## Setting Up

You can either just clone my repository in the description, or create a new project from scratch. If you are creating this project from scratch, then copy these dependencies into a requirements. txt file, and then install them. I recommend using a Python virtual environment for this. Either way, let me quickly walk you through the key dependencies of this project. FastAPI is the actual API framework that you'll be using. Uvicorn is a web server runtime to actually run and host your app locally so that you can test it. Pydantic will be used for data validation, it just makes defining and using API inputs and outputs a little bit easier. And finally, the Google Generative AI package is going to be our AI client to access Gemini. You also need to get an API key for Gemini, and you can do that in a few clicks by going to Google AI Studio. So if you head over to AI Studio, you should see a button in the top here that says "Get API Key". And if you click that, you'll see a button called "Create API Key". Now you might need to set up a Google Cloud project and account, but once you have one, you should just press this button, and you get an API key like this that you can copy and use directly in your project. Now currently, there is a really generous free tier available for this Gemini 2.5 Flash model. So chances are that if you're using this model right now, you can use it for free for experimentation and testing without having to spend a single dollar. Once you have your API key, just set it up as an environment variable like this so that your FastAPI server can access it later. Now with everything set up, let's start building the app.

## Create a FastAPI App

I'm going to walk you through code snippets of the most important ideas for building this app one by one, and then we can go back to the code project and the IDE to see how it all fits together and then how you can run it. But if you wanted to see all of the code of the project put together in one place, then again, I recommend going to this GitHub repository and cloning the project for reference. Okay, so moving on, our first step is to create a main. py file as our entry point to the FastAPI application. And here is the code to actually create a FastAPI application. This app is going to be where we attach all the endpoints to and all of the logic that comes with invoking one of those endpoints. Now before we can write the code for the endpoints, we'll need to define the request and the response models for these endpoints. FastAPI uses pydantic models to automatically handle data validation and serialization. So we're going to create some models for the input and the output of our endpoint, which I'll call a chat request and a chat response. And these both contain a string field. With our request and response models created, we can now define the actual endpoints. And this is how you define an endpoint in FastAPI. You could use this app decorator and then create a function that will be executed when the user visits that endpoint. And here our endpoint doesn't really do anything special. It just returns a message that tells us that the server is running. And at this point, if you just want to test it, you can run the server by using this command in your project folder. So let's go ahead and give that a go. And you should see a message like this saying that your server has started. And here's a link where you can access the server. And so if I open that in my browser, I can see the response from my server here that I wrote in my code. Now the interesting endpoint of our application is going to be the chat endpoint. And this is the code to do it. This will tell FastAPI to listen for post requests at the chat path. The response model parameter tells FastAPI to make sure the response we return matches the chat response structure. And the function below that decorator is what will run when a request comes in. Currently, our function just sends some hard-coded text response back because we haven't implemented the AI integration yet. So let's go ahead and do that now.

## AI Integration (Gemini)

Now the one important thing that I think you need to think about when integrating AI with your apps is how to keep up with how fast these models are improving. To me, this means we want the ability to easily swap out different AI models or maybe try a different model from the same provider really quickly and really easily. And the best way to do that is to abstract the AI implementation away with a simple interface. So here's a definition of a simple interface that we could use for that. This interface says that any class that implements this AI platform base class must have a chat method that takes a string as a prompt and returns a string as a response. Now let's take that interface and use it to implement a concrete class to act as our gemini client. And here is what that looks like. The gemini class takes the API key and an optional system prompt when it is created. The chat method then sends the prompt and prepends the system prompt if it exists to the gemini API and returns the text response. At this point, if you wanted to use a different AI backend for this, all you have to do is just implement this class for that specific backend. Whether it's OpenAI, HuggingFace, or Ollama, it shouldn't take more than 10 minutes with just the few lines of code to replace this component.

## Custom System Prompt

Now if you're making an AI endpoint, it's probably because you have some of your own business logic that you want to add. The easiest way to do this is to define a system prompt in your backend. In this project, I want to steer gemini into providing simple answers and using lots of emojis in its response. So here is my system prompt. Answer the user in plain text, no markdown, but also use lots of emojis and be simple, clear, and concise. Now one option is that I could definitely just hard code this instruction into my server, but it might be a better idea to have this in a separate database or a file that gets loaded in. So here in this project, I've just created a markdown file in this directory, which I'm calling system prompt. And I'm just going to write my system instructions in there. So this is going to be a separate text file to store this command. Then in my FastAPI project, I can use a code snippet like this to just load in the system prompt. This lets me easily configure the system prompt for my app without having to touch the application code itself. And if you wanted to upgrade this even more, you'd probably have this loaded from a database or even another API that manages your system prompts. Anyway, back in our main.py file, let's wire everything up that we just created. We'll start by loading our API key from the environment variable and then loading in our system prompt. Then we'll use that to create an instance of our AI platform client. And then finally, we can go back to our endpoint code and update it to use our AI client and have us send back a real response. Now let me just pop over back to the editor to show you what the project looks like at this stage. In our AI folder, we have our interface for the AI platform. And separately, we have the Gemini client as an implementation of that interface. And here for this model, I'm going to use 2.5 flash. This is a preview model, so it might change by the time you're watching this. But here's a link where you can see and select all the different models and compare their pros and cons. Down here, this is the code itself that actually uses the model. Now if we pop over to the prompts folder, you'll see our system prompt here in the markdown file. And in our main folder, we've got everything put together for the app. We've got all our imports at the top. We create this FastAPI app. We load in the system prompt, get the Gemini API key, and then create our Gemini client. And here we define the request and response data types. And finally, we create the API endpoints. Here's the root endpoint that doesn't really do anything. It just basically gives us a health check. And then we have the star of the show, which is the chat endpoint that uses our AI client to create and send back a response. Now that we've finished building up the core

## Run the Server

functionality, let's run the app again. This command that you've used before will start a local web server, usually at localhost 8000. And the reload flag will help us restart the server whenever you save changes to your code. Once you run the command, you should see something like this, which says that the server is running. And then you can visit the endpoint directly in your web browser. And here we see our root message that we saw before. But what's also handy is you can do /docs. And then you can view this auto-generated documentation for your API endpoints. And you can see we have two endpoints here, one for the root and one for the chat. And this documentation is interactive, so you can actually use this to test this endpoint from right here. Just click "try it out" and then enter your string. So this is a really handy way if you just want to test and inspect the endpoint via the web UI. Alternatively, you could also just call the endpoint via any other endpoint testing tool, such as Postman or cURL. This cURL command will send a post request to our chat endpoint with a JSON body containing this prompt. And as you can see here, it's also following our system instruction, which is to use a lot of emojis and to keep the response simple. Okay, so now that we have our basic API endpoint set

## Rate Limiting

up and talking to our Gemini model, the next thing we need to do is think about how to protect it. Especially if you're planning to offer this API to other users, you'll want to prevent people from making way too many requests and potentially overloading your service or racking up huge bills with your AI provider. So this is where rate limiting comes in. And this is not even always about protecting people who want to harm your service or who want to abuse your service. Sometimes if you build a service and it just gets popular faster and quicker than you anticipated, this can also cause a lot of unexpected load. We can counter this by adding a simple global rate limit to our API. And here's some logic on how to do that, which I'm going to put into a file called throttling. What this code does is it will keep track of how many requests a specific user ID has made within a certain time window, and then it will block it if it exceeds that limit. And since we don't have authentication yet, all unauthenticated users will share the same ID. This is essentially a global limit. And to store the history of that, we're going to use a simple in-memory dictionary for this right now. In reality, you would probably want to use something like a distributed database. But anyway, if a user makes more requests than the global rate limit within the time window, then the API will return as an error. Otherwise, it just records the time of the current request so that it could use it for the next calculation when the request comes in. I've set the global limit to three just to make it easier to test and demonstrate that it works. And I will make this time window reset every 60 seconds. Now we actually need to use this function in our API endpoint. So we'll go back to our main.py file and add a call to that apply rate limit right at the beginning of our chat endpoint. For now, since we don't have individual users, we'll just use a fixed string like global unauthenticated user as the user ID. By adding this line, we've now put a basic rate limit on our chat endpoint. So if we run it again, we should be able to try hitting the endpoint multiple times within 60 seconds. And we should see the error. Working is intended to prevent too many users from overloading this endpoint. So let's go back to our terminal. I've run the server again with the code changes. And now let's send it a couple of requests. That's the first one. That's the second one. Third one. And now on the fourth attempt, I actually get throttled. And I can see this error saying to try again later. And if I go back to the tab with my code, I can also see that I'm printing out the rate limit usage here. And then on the fourth time, we throw too many requests as an error. Of course, if you wait a while for that time window to expire, then we're able to send a request once again, as you can see here. Now that we have a global rate limit in place, we

## Authentication with JWT

might want to think about how we can implement different limits for specific authenticated users. And this is really good if you want to have something like premium access plans available as part of your service as well. To do that, we will need a way to identify who is making that request. And this is where authentication comes in. We're going to add a simple stub for JSON Web Token, or JWT, authentication. This is actually a very common way for you to implement authentication. It's stateless, it's widely supported, and it's secure. Now we're not going to spend too much time going into details about how JWT works, nor will we be implementing the sign-in flow itself. It's quite a deep topic, so if you're not already familiar with that, then I recommend you learning about that first. Instead, we're just going to be using a tool to generate a test token directly, and then focus on how we can use that token inside our backend. The first thing we need to do is to add a library to handle JWTs. There's many options for this, but this library is the one that I'll be using. So add this to your requirements file and install it. Next, we'll need to create a new file for our authentication logic. Inside this file, we'll set up a dependency function that will try to get the user's identity from the request header. Here's a simplified version of the code to do this, and if your goal is just to get this API server running quickly, then it's not super important to understand how all of this works in detail. But let me highlight the other things that we need to do. First, with JWT, the token will be contained in the HTTP request header under the authentication field. It will be in the form of a bearer token, and this OAuth part of the code here is what helps us extract that token value from the HTTP header. Next, your backend will also need a secret key to be able to validate the token. We're going to hard code this key for now, but in production, you'd probably want a much stronger key, or you might even want to move away this validation logic to a separate API completely, which is also pretty common. And notice I say "validate" and not "decode" because by default, anyone can actually decode and view the contents of a JWT token. Nothing in it is secret, but only with the secret key can you determine that the token signature is valid. In other words, the token hasn't been forged and it's not expired. Now down here, the user ID for this token is actually contained in this sub field, and sub stands for subject. So that is going to be the key that we will use for our rate limiter. And finally, if there's no token at all, we'll just consider this as a global unauthenticated user. So this is useful for an app where you might want to have an API that anybody can use to try it out for free before they become a user. Now let's go back to the throttling file to implement this authenticated throttling limit. We're going to add a couple more constants, and we're going to use a separate limit for authenticated users than we're doing for the global rate limit. And then below, in our actual rate limiting function, we're also going to make it take the user ID and actually use that as the key in the user request timestamp dictionary. And we will pick the rate and the time window, depending on whether that key is one of the global users or a specific user. And even though this looks like a whole bunch of code, it's actually a really simple and straightforward modification to the code that we actually had earlier. Now we can go back to our chat endpoint and put everything together. We'll add the user ID to the function signature for the endpoint. FastAPI will automatically call our getUserIdentifier function to unpack the request header and whatever it returns will be passed into the user ID variable. And then we'll just pass that into our rate limiting function. This will set up a really simple basic way for our API to tell the difference between requests that have a valid token and those that don't, allowing us to apply different roles like rate limiting to them. Now if you're implementing this for real, on the front end you would probably have something like sign in with Google or maybe even your own sign up form and that will be the thing that generates the token. So this part we're going to skip that for the tutorial, but normally if you wanted to know how that token is generated, it's usually through that process. And your sign in system and your back end will usually know the same secret key. So that is how your back end can validate whether the token created or sent by the user was a valid token. Even though we're not implementing that front end sign in flow, there's actually still a really simple way that we can test this from the back end. For example, let's say that we wanted to create a test token with this following payload. So here the user ID or the subject is testUser. Now a simple way to do this is to go to jwt.io. This gives you a jwt debugger that is useful for experimenting and testing JWT tokens. So to generate a JWT token, go and click on the encoder tab. And here you're going to have to fill these in with a couple of things. For the header and algorithm section, we want the HS256 algorithm and the type is going to be a JWT token. So copy that in there. And then here just paste in our payload. So that's going to be this one. And then for the JWT secret, just make sure that the string is the same thing that you have inside your project code. So that should be this part here. Now once that all is done, you'll see that it generates a JSON web token for you right here. So you can copy that and then use it as part of your request. Here is an example curl command that you can use to send a request to your endpoint, but this time have it attached with an authentication token inside the header. All you have to do is replace the generated token, the part with a star, with the token you just copied. And then you can try making this request again with your server running. So let's go back to our terminal and give that a go.

## Finished Project

And here I've replaced the token with the one I've generated. And this time running it again, it was successful. Now let me try running it without the token. This still works as well. And if we go back to our server log in the backend, you could see that it's actually authenticating them using a different throttling limit. So my first request was done using this user key for test user, and it used one out of the five requests for that user. And then the second one, it's using it as a global unauthenticated user, so that only had three requests. So you could see that they're actually hitting different limits. Now I'm going to do a couple more with this test user, and then I'll create another user token and try it out, and just to make sure that they're actually using different limit buckets. Okay, so I've made a couple of requests using that original test user. I've created a new token now that uses a different ID for the user. Okay, and then if we go to our backend, you can see that test user here used three out of its five requests. But then with my new token, I'm using a different user, and that one is using a different set of request buckets entirely. With this system, we now have basic authentication and rate limiting for our AI endpoint, which is pretty much table stakes if you wanted to build an application like this. Now just a disclaimer here, authentication and rate limiting are some of the most complex problems you'll ever face in a large-scale distributed system. So in this project, we're just getting a small taste of it. In reality though, it's going to take a lot more work to make the scale. So now you've built a basic fast AI application that serves an AI chat endpoint, and you've integrated Gemini as your AI model. Added rate limiting, and included a stub for authentication. As always, the code for this project is available on GitHub, and the link is in the description. I hope you enjoyed this simple project as a starting point. If you wanted to try and take it a step further, then you could look into how to host a FastAPI application, or how to connect it to a database. I have some videos related to that as well, which I will link in the description. And if you enjoyed this video, then let me know in the comments what other types of topics you'd be interested to see. Otherwise, I'll see you in the next one.
